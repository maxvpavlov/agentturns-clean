# Llama 3.1 Special Tokens Reference

A comprehensive lookup guide for Llama 3.1 special tokens, including tool calling and chat formatting tokens.

---

## Table of Contents
1. [Core Special Tokens](#core-special-tokens)
2. [Chat Template Tokens](#chat-template-tokens)
3. [Tool Calling Tokens](#tool-calling-tokens)
4. [Message Roles](#message-roles)
5. [Token IDs](#token-ids)
6. [Usage Examples](#usage-examples)

---

## Core Special Tokens

### `<|begin_of_text|>`
- **Purpose**: Marks the start of any prompt/text sequence
- **Equivalent**: BOS (Beginning of Sequence) token
- **Usage**: Automatically prepended to all prompts
- **Token ID**: 128000

### `<|end_of_text|>`
- **Purpose**: Signals the model to cease generating tokens
- **Equivalent**: EOS (End of Sequence) token
- **Usage**: Generated by base models to indicate completion
- **When generated**: Primarily by base models, not instruction-tuned models
- **Token ID**: 128001

### `<|finetune_right_pad_id|>`
- **Purpose**: Padding token for batch processing
- **Usage**: Pads text sequences to uniform length in batches
- **Context**: Used during fine-tuning and inference with batched inputs

---

## Chat Template Tokens

### `<|start_header_id|>` and `<|end_header_id|>`
- **Purpose**: Enclose the role identifier for each message turn
- **Format**: `<|start_header_id|>{role}<|end_header_id|>`
- **Valid roles**:
  - `system` - System instructions
  - `user` - User messages
  - `assistant` - Model responses
  - `ipython` - Tool execution results (when using built-in tools)
  - `tool` - Custom tool execution results

**Example**:
```
<|start_header_id|>system<|end_header_id|>

You are a helpful assistant.
```

### `<|eot_id|>` (End of Turn)
- **Purpose**: Marks the end of a complete conversational turn
- **When used**: Standard message completion, no further interaction expected
- **Generation**: Model generates this when it has finished its response
- **Difference from `<|eom_id|>`**: Indicates **final** completion of a turn

**Example**:
```
<|start_header_id|>assistant<|end_header_id|>

Here is my answer to your question.<|eot_id|>
```

---

## Tool Calling Tokens

### `<|eom_id|>` (End of Message)
- **Purpose**: Signals a stopping point where tool execution is needed
- **Introduced**: Llama 3.1 (not in Llama 3.0)
- **When used**: Multi-step reasoning scenarios requiring tool calls
- **Key distinction**: Indicates **continuation expected** (not final completion)
- **Trigger**: Requires `Environment: ipython` in system prompt

**When `<|eom_id|>` vs `<|eot_id|>`**:
- `<|eom_id|>`: "I need a tool to be executed, please give me the result"
- `<|eot_id|>`: "I'm done with this turn completely"

**Example**:
```
<|start_header_id|>assistant<|end_header_id|>

<|python_tag|>get_weather.call(city="Paris")<|eom_id|>
```

### `<|python_tag|>`
- **Purpose**: Special tag marking the start of a tool call in the response
- **Format**: Followed by tool call syntax
- **Used with**: Built-in tools in iPython environment mode
- **Termination**: Always terminated with `<|eom_id|>`, not `<|eot_id|>`

**Built-in Tools Format**:
```python
<|python_tag|>tool_name.call(arg1="value1", arg2="value2")
```

**Custom Tools Format** (JSON):
```json
{"name": "tool_name", "parameters": {"arg1": "value1"}}
```

---

## Message Roles

### 1. `system`
- **Purpose**: Provide instructions and context to the model
- **When to use**: First message, sets behavior and capabilities
- **Tool calling**: Include `Environment: ipython` to enable tool use

**Example**:
```
<|start_header_id|>system<|end_header_id|>

Environment: ipython
You are a helpful assistant with access to tools.
Use tools when needed.<|eot_id|>
```

### 2. `user`
- **Purpose**: User's input/query
- **Tool calling**: Can include tool descriptions in first user message

### 3. `assistant`
- **Purpose**: Model's responses
- **Can contain**: Regular text, tool calls, or both
- **Termination**: Either `<|eot_id|>` (done) or `<|eom_id|>` (needs tool execution)

### 4. `ipython` / `tool`
- **Purpose**: Return tool execution results to the model
- **Usage**: After model generates `<|eom_id|>`, executor provides results
- **`ipython`**: Used for built-in tools
- **`tool`**: Used for custom tools

**Example**:
```
<|start_header_id|>ipython<|end_header_id|>

{"temperature": 22, "condition": "sunny"}<|eot_id|>
```

---

## Token IDs

Reference for common special token IDs in Llama 3.1:

| Token | ID | Hex |
|-------|-----|-----|
| `<|begin_of_text|>` | 128000 | 0x1F400 |
| `<|end_of_text|>` | 128001 | 0x1F401 |
| `<|start_header_id|>` | 128006 | 0x1F406 |
| `<|end_header_id|>` | 128007 | 0x1F407 |
| `<|eot_id|>` | 128009 | 0x1F409 |
| `<|eom_id|>` | 128008 | 0x1F408 |
| `<|python_tag|>` | 128010 | 0x1F40A |
| `<|reserved_special_token_0|>` | 128002 | 0x1F402 |
| `<|reserved_special_token_255|>` | 128257 | 0x1F501 |

**Note**: Llama 3.1 reserves 256 special tokens (`<|reserved_special_token_0|>` through `<|reserved_special_token_255|>`) for future use.

---

## Usage Examples

### Standard Chat (No Tools)

```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>

What is the capital of France?<|eot_id|><|start_header_id|>assistant<|end_header_id|>

The capital of France is Paris.<|eot_id|>
```

### Tool Calling with Built-in Tools

```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

Environment: ipython
Tools: get_weather
You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>

What's the weather in Paris?<|eot_id|><|start_header_id|>assistant<|end_header_id|>

<|python_tag|>get_weather.call(city="Paris")<|eom_id|><|start_header_id|>ipython<|end_header_id|>

{"temperature": 22, "condition": "sunny"}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

The weather in Paris is currently sunny with a temperature of 22°C.<|eot_id|>
```

### Tool Calling with Custom Tools (JSON Format)

```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a helpful assistant with access to tools.<|eot_id|><|start_header_id|>user<|end_header_id|>

Given the following functions, please respond with a JSON for a function call.
[tool definitions here]

What's the weather in Paris?<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{"name": "get_weather", "parameters": {"city": "Paris"}}<|eot_id|><|start_header_id|>tool<|end_header_id|>

{"temperature": 22, "condition": "sunny"}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

The weather in Paris is currently sunny with a temperature of 22°C.<|eot_id|>
```

### Multi-step Tool Calling

```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

Environment: ipython
Tools: search, calculator
You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>

What is the population of Tokyo multiplied by 2?<|eot_id|><|start_header_id|>assistant<|end_header_id|>

<|python_tag|>search.call(query="population of Tokyo")<|eom_id|><|start_header_id|>ipython<|end_header_id|>

{"result": "14 million"}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

<|python_tag|>calculator.call(expression="14000000 * 2")<|eom_id|><|start_header_id|>ipython<|end_header_id|>

{"result": 28000000}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

The population of Tokyo (14 million) multiplied by 2 equals 28 million.<|eot_id|>
```

---

## Key Implementation Notes

### Tool Calling Setup

1. **For built-in tools**: Add `Environment: ipython` to system prompt
2. **For custom tools**: Provide tool definitions in user message
3. **Tool results**: Return as `ipython` or `tool` role messages

### Important Distinctions

- **`<|eom_id|>` = "Continue, need tool execution"**
- **`<|eot_id|>` = "Done, no more interaction needed"**
- Only use `<|eom_id|>` when `Environment: ipython` is set
- Custom tools typically use JSON format without `<|python_tag|>`

### Model Behavior

- Model trained to output `<|python_tag|>` for tool calls in iPython mode
- Without `Environment: ipython`, model outputs JSON directly
- `<|eom_id|>` signals to execution environment: "run this tool and give me results"
- After receiving tool results, model continues reasoning and may call more tools

---

## References

- [Official Llama 3.1 Documentation](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/)
- [Llama Models GitHub - Prompt Format](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/prompt_format.md)
- [Tool Calling Guide - Analytics Vidhya](https://www.analyticsvidhya.com/blog/2024/07/tool-calling-in-llama-3-1/)

---

**Last Updated**: October 2025
**Model Version**: Llama 3.1 / 3.3
**Vocabulary Size**: 128,256 tokens (including 256 reserved special tokens)
