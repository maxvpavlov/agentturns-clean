import subprocess
import sys
import json
from llama_cpp import Llama
from rich.console import Console
from rich.panel import Panel
from rich.syntax import Syntax
from rich.rule import Rule

console = Console()

# Define available tools
def run_shell_command(command: str):
    """
    Execute a shell command and return its output.

    Args:
        command: The shell command to execute.

    Returns:
        str: The output of the command, or an error message.
    """
    try:
        result = subprocess.run(command, shell=True, capture_output=True, text=True, timeout=30)
        if result.returncode == 0:
            return result.stdout.strip()
        else:
            return f"Error: {result.stderr.strip()}"
    except subprocess.TimeoutExpired:
        return "action execution failed with timeout"
    except Exception as e:
        return f"Exception: {str(e)}"

# Map tool names to their actual functions for execution
TOOLS = {
    "run_shell_command": run_shell_command
}

# Tool schema for llama-cpp
TOOL_SCHEMA = [
    {
        "type": "function",
        "function": {
            "name": "run_shell_command",
            "description": "Execute a shell command and return its output.",
            "parameters": {
                "type": "object",
                "properties": {
                    "command": {
                        "type": "string",
                        "description": "The shell command to execute.",
                    },
                },
                "required": ["command"],
            },
        },
    }
]

SYSTEM_PROMPT_PLANNING = """You are a helpful AI assistant.
Create a concise, step-by-step plan to answer the user's question.
Output ONLY the plan as a numbered list. Do not execute any actions yet."""

SYSTEM_PROMPT_ACTING = """You are a helpful AI assistant with access to tools.
Execute the plan step-by-step. For each plan item:
1. Call the appropriate tool to gather information
2. Wait for the result
3. Move to the next step

IMPORTANT:
- Call ONE tool at a time (not multiple tools in one response)
- After receiving a tool result, decide if you need more tools or are done
- When all necessary information is gathered, respond with text (no tool call) to indicate completion"""

SYSTEM_PROMPT_SYNTHESIS = """You are a helpful AI assistant.
Review the conversation history including the original question and all tool results.
Provide a clear, direct answer to the user's original question based on the information gathered.
Do NOT describe what tools were used or suggest calling more tools.
Just answer the question concisely."""

# NOTE: Special tokens like <|eot_id|>, <|eom_id|>, etc. are generated by the model
# but are abstracted away by llama-cpp-python. They are not visible in the response.
# finish_reason="stop" indicates an EOG token (typically <|eot_id|>) was generated.
# See TOKEN_ABSTRACTION_IN_LLAMA_CPP_PYTHON.md for details.

def load_model():
    """Load the Llama model with tool calling support."""
    console.print("[bold green]Loading model...[/bold green]")
    try:
        llm = Llama(
            model_path="Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
            n_ctx=8192,  # Context window
            n_gpu_layers=-1,  # Full GPU offload if available
            verbose=False
        )
        console.print("[bold green]Model loaded successfully![/bold green]")
        return llm
    except Exception as e:
        console.print(f"[bold red]Error loading model: {e}[/bold red]")
        console.print("[yellow]Make sure Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf is in the current directory[/yellow]")
        sys.exit(1)

def run_agent(llm, query, max_steps=10):
    """Main agent loop using ReAct pattern with native tool calling."""

    # PHASE 1: PLANNING (no tools available)
    console.print(Rule("[bold blue]Step 1: Planning"))

    planning_messages = [
        {"role": "system", "content": SYSTEM_PROMPT_PLANNING},
        {"role": "user", "content": query}
    ]

    with console.status("[bold green]Creating plan..."):
        planning_response = llm.create_chat_completion(
            messages=planning_messages,
            temperature=0.7,
            max_tokens=1024
        )

    plan = planning_response["choices"][0]["message"]["content"]
    console.print(Panel(plan, title="Plan", border_style="cyan"))

    # PHASE 2: EXECUTION (with native tool calling)
    execution_messages = [
        {"role": "system", "content": SYSTEM_PROMPT_ACTING},
        {"role": "user", "content": f"Original question: {query}\n\nPlan to execute:\n{plan}\n\nExecute the first step of the plan."}
    ]

    tool_uses = 0
    max_tool_uses = 10  # Safety limit to prevent infinite loops

    for step in range(2, max_steps + 1):
        if tool_uses >= max_tool_uses:
            console.print(Panel(f"Reached maximum of {max_tool_uses} tool uses. Proceeding to synthesis.",
                              title="Info", border_style="blue"))
            break

        console.print(Rule(f"[bold blue]Step {step}"))

        # Call model with tools
        with console.status("[bold green]Thinking..."):
            try:
                response = llm.create_chat_completion(
                    messages=execution_messages,
                    tools=TOOL_SCHEMA,
                    tool_choice="auto",
                    temperature=0.7,
                    max_tokens=2048
                )
            except Exception as e:
                console.print(Panel(f"Error during model inference: {e}", title="Error", border_style="bold red"))
                return "Agent stopped due to error."

        response_message = response["choices"][0]["message"]
        content = response_message.get("content", "")

        # Log raw content for debugging
        console.print(Panel(content, title="Raw Model Response", border_style="blue", expand=False))

        # Llama 3.1 returns tool calls as JSON in the content field
        # Try to parse content as a tool call
        parsed_tool_call = None
        try:
            parsed = json.loads(content.strip())
            if isinstance(parsed, dict) and "name" in parsed:
                # This is a tool call in JSON format (Llama 3.1 native format)
                parsed_tool_call = parsed
        except json.JSONDecodeError:
            # Not JSON - this is a regular text response, model is done with tools
            pass

        if parsed_tool_call:
            # Model wants to use a tool
            tool_uses += 1
            function_name = parsed_tool_call.get("name")
            function_args = parsed_tool_call.get("parameters", {})

            if function_name in TOOLS:
                command_to_run = function_args.get("command", "")
                syntax = Syntax(command_to_run, "bash", theme="monokai", line_numbers=True)
                console.print(Panel(syntax, title=f"Action: {function_name}", border_style="dark_orange"))

                observation = TOOLS[function_name](command_to_run)
                console.print(Panel(observation, title="Observation", border_style="green"))

                # Add tool call and result to history
                execution_messages.append({"role": "assistant", "content": content})
                execution_messages.append({
                    "role": "user",
                    "content": f"Tool result: {observation}\n\nContinue with the next step of the plan, or indicate completion if done."
                })
            else:
                error_msg = f"Error: Unknown tool: {function_name}"
                console.print(Panel(error_msg, title="Error", border_style="bold red"))
                execution_messages.append({"role": "assistant", "content": content})
                execution_messages.append({"role": "user", "content": error_msg})
        else:
            # Regular text response - model is done with tools
            if content:
                console.print(Panel(content, title="Completion Message", border_style="yellow"))
                execution_messages.append({"role": "assistant", "content": content})
            break

    # PHASE 3: SYNTHESIS - Generate final answer from gathered information
    console.print(Rule("[bold blue]Final Step: Synthesis"))

    synthesis_messages = [
        {"role": "system", "content": SYSTEM_PROMPT_SYNTHESIS},
        {"role": "user", "content": f"Original question: {query}\n\nHere is what happened:\n{plan}\n\nNow provide a clear, direct answer to the original question."}
    ]

    # Add tool interactions to synthesis context
    for msg in execution_messages[2:]:  # Skip system and initial user message
        if msg.get("role") == "assistant" and msg.get("content"):
            synthesis_messages.append({"role": "assistant", "content": msg["content"]})
        elif msg.get("role") == "user":
            synthesis_messages.append({"role": "user", "content": msg["content"]})

    with console.status("[bold green]Synthesizing final answer..."):
        try:
            final_response = llm.create_chat_completion(
                messages=synthesis_messages,
                temperature=0.3,  # Lower temperature for more focused answer
                max_tokens=1024
            )
        except Exception as e:
            console.print(Panel(f"Error during synthesis: {e}", title="Error", border_style="bold red"))
            return "Agent stopped due to error."

    final_answer = final_response["choices"][0]["message"]["content"]
    console.print(Panel(final_answer, title="Final Answer", border_style="sky_blue1"))
    return final_answer

if __name__ == "__main__":
    if len(sys.argv) > 1:
        query = sys.argv[1]

        # Load model once
        llm = load_model()

        # Run agent
        result = run_agent(llm, query)
        console.print(Rule("[bold magenta]Result"))
        console.print(result)
    else:
        print("Usage: python 8.agent-llama_cpp-tool-calling.py 'your query'")
